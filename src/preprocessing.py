# Third-Party Imports
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline


def preprocess():
    # load the training and testing data
    print("\tLoading in the dataset...")
    data = pd.read_csv('../data/pe_imports_subset_1k.csv')

    # separate categorical, numerical, and target columns
    # categorical_columns = ['hash']
    numeric_columns = data.drop(['hash', 'malware'], axis=1).columns.tolist()
    target_column = ['malware']

    # TODO: Dropping hash value for now (re-add later)
    data = data.drop('hash', axis=1)

    # create transformers for standard scaling
    numeric_transformer = Pipeline(steps=[
        ('scaler', StandardScaler())
    ])

    # combine transformers using ColumnTransformer for feature columns
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numeric_columns)
        ], remainder='passthrough'  # This includes the target variable without transformation
    )

    # extract and keep the target variable
    y = data[target_column]

    # apply preprocessing to the feature columns
    print("\tProcessing the training data...")
    X_transformed = preprocessor.fit_transform(data.drop(target_column, axis=1))

    # # combine the categorical feature names with numeric column names
    # feature_names = list(categorical_feature_names) + numeric_columns

    # convert the transformed array back to a DataFrame with appropriate column names
    transformed_df = pd.DataFrame(X_transformed, columns=numeric_columns)

    # concatenate the feature DataFrame with the target variable
    result = pd.concat([transformed_df, y], axis=1)

    # send the resulting dataframe to a CSV file
    print("\tWriting the data to a CSV file...")
    result.to_csv(r'../data/training_data.csv', index=False)
    print("\tWrote the data to ./data/training_data.csv\n")
